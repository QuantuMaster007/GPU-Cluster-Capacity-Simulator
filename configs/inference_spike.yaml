seed: 123

cluster:
  name: "inference-spike"
  nodes: 6
  gpus_per_node: 8
  gpu_type: "H100"
  interconnect: "NVLINK"
  efficiency_factor: 0.90

pricing:
  gpu_hour_cost_usd: 3.25
  overhead_multiplier: 1.20

simulation:
  duration_minutes: 720
  time_step_minutes: 1

scheduler:
  policy: "FFD"
  enable_inference_priority: true
  enable_preemption: false

workload:
  job_streams:
    - name: "training-baseline"
      job_type: "training"
      arrivals_per_hour: 4
      gpus_required_min: 2
      gpus_required_max: 8
      duration_minutes_min: 60
      duration_minutes_max: 180
      priority: 2

    - name: "inference-high"
      job_type: "inference"
      arrivals_per_hour: 80
      gpus_required_min: 1
      gpus_required_max: 2
      duration_minutes_min: 2
      duration_minutes_max: 8
      priority: 1
      sla_wait_minutes: 2
